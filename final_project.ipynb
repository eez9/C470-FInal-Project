{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ],
      "metadata": {
        "id": "uxgBX0YXu1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score"
      ],
      "metadata": {
        "id": "WVsHoZltF3sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "FXyRfd35yRPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ],
      "metadata": {
        "id": "5jRvHTlW0DYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ],
      "metadata": {
        "id": "OUzUupva0Fxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ],
      "metadata": {
        "id": "mAssSW_I0GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "trainData, testData = train_test_split(file, test_size=0.2, random_state=19)\n",
        "\n",
        "fTrn = trainData.iloc[:, :-1].values\n",
        "sTrn = trainData.iloc[:, -1].values\n",
        "fTest = testData.iloc[:, :-1].values\n",
        "sTest = testData.iloc[:, -1].values\n",
        "\n",
        "class NaiveBayes:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.prProb = None\n",
        "    self.cndProb = None\n",
        "\n",
        "  def learn(self, xVal, yVal):\n",
        "    self.prProb = {}\n",
        "    cType, index = np.unique(yVal, return_counts=True)\n",
        "    samp = len(yVal)\n",
        "    for types, count in zip(cType, index):\n",
        "      self.prProb[types] = count / samp\n",
        "\n",
        "      self.cndProb = {}\n",
        "      for types in cType:\n",
        "        data = xVal[yVal == types]\n",
        "        wordCount = np.sum(data > 0, axis=0) + 1\n",
        "        self.cndProb[types] = wordCount / (len(data) + 2)\n",
        "\n",
        "  def predict(self, val):\n",
        "    prediction = []\n",
        "    for data in val:\n",
        "      posts = {}\n",
        "      for types in self.prProb:\n",
        "        outcome = np.prod(self.cndProb[types][data > 0])\n",
        "        post = outcome * self.prProb[types]\n",
        "        posts[types] = post\n",
        "      prediction.append(max(posts, key=posts.get))\n",
        "    return prediction\n",
        "'''\n",
        "class logisticRegression:\n",
        "\n",
        "  def __init__(self, rate=0.01, iterations=1000):\n",
        "    self.rate = rate\n",
        "    self.iterations = iterations\n",
        "    self.bias = None\n",
        "    self.weight = None\n",
        "\n",
        "  def learn(self, xVal, yVal):\n",
        "    feature, sample = xVal.shape\n",
        "    self.weight = np.zeros(feature)\n",
        "    self.bias = 0\n",
        "\n",
        "    for _ in range(self.iterations):\n",
        "      lineMod = np.dot(self.weight, xVal) + self.bias\n",
        "      prediction = 1/(1 + np.exp(lineMod))\n",
        "      gradientW = (1/ sample) * np.dot(xVal.T, (prediction - yVal))\n",
        "      gradientB = (1/ sample) * np.sum(prediction - yVal)\n",
        "\n",
        "      self.weight -= self.rate * gradientW\n",
        "      self.bias -= self.rate * gradientB\n",
        "\n",
        "  def predict(self, value):\n",
        "    lineMod = np.dot(value, self.weight) + self.bias\n",
        "    prediction = 1/(1 + np.exp(-lineMod))\n",
        "\n",
        "    types = []\n",
        "    for prob in prediction:\n",
        "      if prob > 0.5:\n",
        "        types.append(1)\n",
        "      else:\n",
        "        types.append(0)\n",
        "    return types\n",
        "'''\n",
        "class KNN:\n",
        "\n",
        "  def __init__(self, value = 3):\n",
        "    self.value = value\n",
        "\n",
        "\n",
        "  def knn(self, xVal, yVal):\n",
        "    self.xVal = xVal\n",
        "    self.yVal = yVal\n",
        "\n",
        "  def predict( self, xVal):\n",
        "    yVal = []\n",
        "\n",
        "    for val in xVal:\n",
        "      sample = self.learn(val)\n",
        "      yVal.append(sample)\n",
        "\n",
        "    return np.array(yVal)\n",
        "\n",
        "  def learn( self, val):\n",
        "    distances = []\n",
        "\n",
        "    for train in self.xVal:\n",
        "      distance = np.sqrt(np.sum((val - train) ** 2))\n",
        "      distances.append(distance)\n",
        "\n",
        "    index = np.argsort(distance)[:self.value]\n",
        "\n",
        "    kLabel = []\n",
        "\n",
        "    for num in index:\n",
        "      label = self.yVal[num]\n",
        "      kLabel.append(label)\n",
        "\n",
        "    comLable = np.bincount(kLabel).argmax()\n",
        "\n",
        "    return comLable\n",
        "\n",
        "\n",
        "\n",
        "navB = NaiveBayes()\n",
        "navB.learn(fTrn, sTrn)\n",
        "prediction = navB.predict(fTest)\n",
        "'''\n",
        "logReg = logisticRegression()\n",
        "logReg.learn(fTrn, sTrn)\n",
        "logRegPred = logReg.predict(fTest)\n",
        "'''\n",
        "KNN = KNN()\n",
        "KNN.knn(fTrn, sTrn)\n",
        "KNNPred = KNN.predict(fTest)\n",
        "\n",
        "accuracy = accuracy_score(sTest, prediction)\n",
        "confMatrix = confusion_matrix(sTest, prediction)\n",
        "truePos = confMatrix[1,1]\n",
        "falsePos = confMatrix[0,1]\n",
        "AUC = roc_auc_score(sTest, navB.predict(fTest))\n",
        "'''\n",
        "logRegAcc = accuracy_score(sTest, logRegPred)\n",
        "logRegConfMatrix = confusion_matrix(sTest, logRegPred)\n",
        "logRegTruePos = logRegConfMatrix[1,1]\n",
        "logRegFalsePos = logRegConfMatrix[0,1]\n",
        "logRegAUC = roc_auc_score(sTest, logReg.predict(fTest))\n",
        "'''\n",
        "knnAcc = accuracy_score(sTest, KNNPred)\n",
        "knnConfMatrix = confusion_matrix(sTest, KNNPred)\n",
        "knnTruePos = knnConfMatrix[1,1]\n",
        "knnFalsePos = knnConfMatrix[0,1]\n",
        "knnAUC = roc_auc_score(sTest, KNN.predict(fTest))\n",
        "\n",
        "\n",
        "print(\"Naive Bayes:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"True Positives:\", truePos)\n",
        "print(\"False Positives:\", falsePos)\n",
        "print(\"Area Under Curve:\", AUC)\n",
        "print(\"-------------------------------\")\n",
        "print()\n",
        "'''print(\"Logistic Regression:\")\n",
        "print(\"Accuracy:\", logRegAcc)\n",
        "print(\"True Positives:\", logRegTruePos)\n",
        "print(\"False Positives:\", logRegFalsePos)\n",
        "print(\"Area Under Curve:\", logRegAUC)\n",
        "print(\"-------------------------------\")\n",
        "print()'''\n",
        "print(\"KNN:\")\n",
        "print(\"Accuracy:\", knnAcc)\n",
        "print(\"True Positives:\", knnTruePos)\n",
        "print(\"False Positives:\", knnFalsePos)\n",
        "print(\"Area Under Curve:\", knnAUC)\n"
      ],
      "metadata": {
        "id": "e0MQ0eo0MnmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090cab73-99e7-4ca0-874e-820237a214cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes:\n",
            "Accuracy: 0.8197611292073833\n",
            "True Positives: 362\n",
            "False Positives: 157\n",
            "Area Under Curve: 0.8451433472188189\n",
            "-------------------------------\n",
            "\n",
            "KNN:\n",
            "Accuracy: 0.5971769815418024\n",
            "True Positives: 0\n",
            "False Positives: 0\n",
            "Area Under Curve: 0.5\n"
          ]
        }
      ]
    }
  ]
}